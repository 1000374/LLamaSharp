<Project Sdk="Microsoft.NET.Sdk">
  <PropertyGroup>
    <TargetFrameworks>netstandard2.0;net6.0;net8.0</TargetFrameworks>
    <RootNamespace>LLama</RootNamespace>
    <Nullable>enable</Nullable>
    <LangVersion>12</LangVersion>
    <Platforms>AnyCPU;x64;Arm64</Platforms>
    <AllowUnsafeBlocks>True</AllowUnsafeBlocks>

    <Version>0.13.0</Version>
    <Authors>Rinne, Martin Evans, jlsantiago and all the other contributors in https://github.com/SciSharp/LLamaSharp/graphs/contributors.</Authors>
    <Company>SciSharp STACK</Company>
    <GeneratePackageOnBuild>true</GeneratePackageOnBuild>
    <Copyright>MIT, SciSharp STACK $([System.DateTime]::UtcNow.ToString(yyyy))</Copyright>
    <RepositoryUrl>https://github.com/SciSharp/LLamaSharp</RepositoryUrl>
    <RepositoryType>git</RepositoryType>
    <PackageIconUrl>https://avatars3.githubusercontent.com/u/44989469?s=200&amp;v=4</PackageIconUrl>
    <PackageTags>LLama, LLM, GPT, ChatGPT, NLP, AI, Chat Bot, SciSharp</PackageTags>
    <Description>
      LLamaSharp is a cross-platform library to run ðŸ¦™LLaMA/LLaVA model (and others) in your local device. 
      Based on [llama.cpp](https://github.com/ggerganov/llama.cpp), inference with LLamaSharp is efficient on both CPU and GPU. 
      With the higher-level APIs and RAG support, it's convenient to deploy LLM (Large Language Model) in your application with LLamaSharp.
    </Description>
    <PackageReleaseNotes>
      Updated llama.cpp version to include better support for LLama3 tokenization.
    </PackageReleaseNotes>
    <PackageLicenseExpression>MIT</PackageLicenseExpression>
    <PackageOutputPath>packages</PackageOutputPath>
    <Platforms>AnyCPU;x64;Arm64</Platforms>
    <PackageId>LLamaSharp</PackageId>
    <Configurations>Debug;Release;GPU</Configurations>
    <GenerateAssemblyInfo>false</GenerateAssemblyInfo>
  </PropertyGroup>

  <PropertyGroup>
    <PackageReadmeFile>README.md</PackageReadmeFile>
    <GenerateDocumentationFile>True</GenerateDocumentationFile>
  </PropertyGroup>

  <ItemGroup>
    <None Include="..\README.md" Pack="true" PackagePath="\" />
  </ItemGroup>

  <ItemGroup Condition="'$(TargetFramework)' == 'netstandard2.0'">
    <PackageReference Include="IsExternalInit" Version="1.0.3" PrivateAssets="all" />
    <PackageReference Include="System.Memory" Version="4.5.5" PrivateAssets="all" />
    <PackageReference Include="System.Linq.Async" Version="6.0.1" />
    <PackageReference Include="System.Text.Json" Version="8.0.3" />
  </ItemGroup>

  <ItemGroup>
    <PackageReference Include="Microsoft.Extensions.Logging.Abstractions" Version="8.0.1" />
  </ItemGroup>

    <PropertyGroup>
        <LlamaCppReleaseTag>b3289</LlamaCppReleaseTag>
    </PropertyGroup>

    <ItemGroup>
        <BinDownload Include="bin-win-noavx-x64" DestFolder="" />
        <BinDownload Include="bin-win-avx-x64" DestFolder="avx" />
        <BinDownload Include="bin-win-avx2-x64" DestFolder="avx2" />
        <BinDownload Include="bin-win-avx512-x64" DestFolder="avx512" />
        <BinDownload Include="bin-win-cuda-cu11.7.1-x64" DestFolder="cu11.7.1" />
        <BinDownload Include="bin-win-cuda-cu12.2.0-x64" DestFolder="cu12.2.0" />
        <BinDownload Include="bin-macos-arm64" DestFolder="osx-arm64" />
        <BinDownload Include="bin-macos-x64" DestFolder="osx-x64" />
        <BinDownload Include="bin-win-vulkan-x64" DestFolder="vulkan" />
    </ItemGroup>

    <ItemGroup>
      <Folder Include="runtimes\deps\**" />
    </ItemGroup>

    <Target Name="DownloadReleaseBinaries" BeforeTargets="DispatchToInnerBuilds">
        <Message Importance="High" Text="Download 'llama-$(LlamaCppReleaseTag)-%(BinDownload.Identity).zip' to 'runtimes/deps/'" />
        <DownloadFile
            SourceUrl="https://github.com/ggerganov/llama.cpp/releases/download/$(LlamaCppReleaseTag)/llama-$(LlamaCppReleaseTag)-%(BinDownload.Identity).zip"
            DestinationFolder="runtimes/deps"
            DestinationFileName="llama-%(BinDownload.Identity).zip"
            SkipUnchangedFiles="true"
        />
    </Target>
    <Target Name="UnpackReleaseBinaries" BeforeTargets="DispatchToInnerBuilds" DependsOnTargets="DownloadReleaseBinaries">
        <Message Importance="High" Text="Unzip 'llama-$(LlamaCppReleaseTag)-%(BinDownload.Identity).zip' to 'runtimes/deps/%(BinDownload.DestFolder)'" />
        <Unzip
            SourceFiles="runtimes/deps/llama-%(BinDownload.Identity).zip"
            DestinationFolder="runtimes/deps/%(BinDownload.DestFolder)"
            OverwriteReadOnlyFiles="false"
            Include="*.dll"
        />
    </Target>




</Project>